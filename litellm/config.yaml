# Local AI - LiteLLM Proxy Configuration
#
# Load-balancing proxy for GPU inference servers.
# Uses PostgreSQL-backed API key management (native LiteLLM).
#
# GPU Servers (PEA - 192.168.0.144):
#   SFW:   GPU 1-3, ports 8080-8082 (Stheno v3.4 Q5_K_M)
#   NSFW:  GPU 4-7, ports 8083-8086 (Josiefied-Qwen3-8B Q5_K_M)
#   Embed: GPU 1-7, ports 8090-8096 (nomic-embed-text-v1.5)
#   Image: GPU 8, port 5100 (Segmind SSD-1B via LocalAI)

model_list:
  # =============================================================================
  # SFW MODELS - 3x GPU load balancing (GPU 1-3)
  # Public API names: heartcode-default, heartcode-sfw
  # Internal name: heartcode-chat-sfw
  # =============================================================================

  - model_name: "heartcode-chat-sfw"
    litellm_params:
      model: "openai/Llama-3.1-8B-Stheno-v3.4-Q5_K_M.gguf"
      api_base: "http://192.168.0.144:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "heartcode-gpu1"
      mode: "chat"

  - model_name: "heartcode-chat-sfw"
    litellm_params:
      model: "openai/Llama-3.1-8B-Stheno-v3.4-Q5_K_M.gguf"
      api_base: "http://192.168.0.144:8081/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "heartcode-gpu2"
      mode: "chat"

  - model_name: "heartcode-chat-sfw"
    litellm_params:
      model: "openai/Llama-3.1-8B-Stheno-v3.4-Q5_K_M.gguf"
      api_base: "http://192.168.0.144:8082/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "heartcode-gpu3"
      mode: "chat"

  # =============================================================================
  # NSFW MODELS - 4x GPU load balancing (GPU 4-7)
  # Public API name: heartcode-nsfw
  # Internal name: heartcode-chat-nsfw
  # =============================================================================

  - model_name: "heartcode-chat-nsfw"
    litellm_params:
      model: "openai/Josiefied-Qwen3-8B-abliterated-v1-q5_k_m.gguf"
      api_base: "http://192.168.0.144:8083/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "heartcode-gpu4"
      mode: "chat"

  - model_name: "heartcode-chat-nsfw"
    litellm_params:
      model: "openai/Josiefied-Qwen3-8B-abliterated-v1-q5_k_m.gguf"
      api_base: "http://192.168.0.144:8084/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "heartcode-gpu5"
      mode: "chat"

  - model_name: "heartcode-chat-nsfw"
    litellm_params:
      model: "openai/Josiefied-Qwen3-8B-abliterated-v1-q5_k_m.gguf"
      api_base: "http://192.168.0.144:8085/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "heartcode-gpu6"
      mode: "chat"

  - model_name: "heartcode-chat-nsfw"
    litellm_params:
      model: "openai/Josiefied-Qwen3-8B-abliterated-v1-q5_k_m.gguf"
      api_base: "http://192.168.0.144:8086/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "heartcode-gpu7"
      mode: "chat"

  # =============================================================================
  # EMBEDDING MODELS - 7x GPU load balancing (GPU 1-7)
  # For RAG/semantic search
  # =============================================================================

  - model_name: "heartcode-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5.Q8_0.gguf"
      api_base: "http://192.168.0.144:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "heartcode-embed1"
      mode: "embedding"

  - model_name: "heartcode-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5.Q8_0.gguf"
      api_base: "http://192.168.0.144:8091/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "heartcode-embed2"
      mode: "embedding"

  - model_name: "heartcode-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5.Q8_0.gguf"
      api_base: "http://192.168.0.144:8092/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "heartcode-embed3"
      mode: "embedding"

  - model_name: "heartcode-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5.Q8_0.gguf"
      api_base: "http://192.168.0.144:8093/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "heartcode-embed4"
      mode: "embedding"

  - model_name: "heartcode-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5.Q8_0.gguf"
      api_base: "http://192.168.0.144:8094/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "heartcode-embed5"
      mode: "embedding"

  - model_name: "heartcode-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5.Q8_0.gguf"
      api_base: "http://192.168.0.144:8095/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "heartcode-embed6"
      mode: "embedding"

  - model_name: "heartcode-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5.Q8_0.gguf"
      api_base: "http://192.168.0.144:8096/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "heartcode-embed7"
      mode: "embedding"

# Router settings for load balancing
router_settings:
  routing_strategy: "least-busy"
  num_retries: 2
  retry_after: 3
  timeout: 90
  allowed_fails: 2
  cooldown_time: 60

  # Health check settings
  enable_health_check: true

  # Rate limiting per deployment to prevent GPU overload
  model_rate_limits:
    - model_name: "heartcode-chat-sfw"
      tpm: 50000
      rpm: 35
    - model_name: "heartcode-chat-nsfw"
      tpm: 50000
      rpm: 45

  # Model aliases - map public API names to internal model names
  model_group_alias:
    "heartcode-default": "heartcode-chat-sfw"
    "heartcode-sfw": "heartcode-chat-sfw"
    "heartcode-nsfw": "heartcode-chat-nsfw"
    "heartcode-chat": "heartcode-chat-sfw"

# General settings - PostgreSQL-backed API key management
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

  # Concurrency limits
  # 7 chat GPUs * 2 concurrent requests each = 14 max safe
  max_parallel_requests: 7
  global_max_parallel_requests: 14

  # Request timeout
  request_timeout: 90

  # Logging
  json_logs: true

  # CORS
  allow_credentials: true

  # Health check interval (seconds)
  health_check_interval: 15

# LiteLLM settings
litellm_settings:
  set_verbose: false
  drop_params: true  # Drop unsupported params instead of erroring

# Environment variables needed:
#   LITELLM_MASTER_KEY - Master key for admin API access
#   DATABASE_URL - PostgreSQL connection string (set in docker-compose)
#   LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_HOST - Optional observability
