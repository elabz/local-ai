# HeartCode Monitoring Setup

## Overview
- **Prometheus**: Runs on GPU server (192.168.0.145:9090), collects metrics from all 8 GPU servers
- **Grafana**: Runs on development server (localhost:3001), visualizes Prometheus data

## Quick Start

### Start Grafana
```bash
cd /Users/boss/Web/heartcode/infrastructure/docker
docker compose -f docker-compose.dev.yml up -d grafana
```

### Access Grafana
- **URL**: http://localhost:3001
- **Username**: admin
- **Password**: admin (change on first login)

### Pre-configured Dashboard
- Navigate to **Dashboards** → **GPU Servers Monitoring**
- Auto-refreshes every 10 seconds

## Available Metrics

### GPU Metrics (per GPU)
- `gpu_memory_used_bytes` - VRAM usage
- `gpu_memory_total_bytes` - Total VRAM
- `gpu_utilization_percent` - GPU utilization
- `gpu_temperature_celsius` - Temperature

### Inference Metrics
- `inference_requests_total` - Total requests by endpoint/status
- `inference_duration_seconds` - Request duration histogram
- `inference_tokens_total` - Tokens generated by type
- `active_requests_gauge` - Current active requests
- `model_loaded` - Model loading status (1=loaded, 0=not loaded)

## Useful PromQL Queries

### GPU Memory Usage (%)
```promql
gpu_memory_used_bytes / gpu_memory_total_bytes * 100
```

### Requests per Second
```promql
rate(inference_requests_total[5m])
```

### Average Inference Time
```promql
rate(inference_duration_seconds_sum[5m]) / rate(inference_duration_seconds_count[5m])
```

### Tokens per Second
```promql
rate(inference_tokens_total[5m])
```

### P95 Inference Latency
```promql
histogram_quantile(0.95, rate(inference_duration_seconds_bucket[5m]))
```

### Total Models Online
```promql
sum(model_loaded)
```

## Creating Custom Dashboards

1. Click **+** → **Dashboard** in Grafana
2. Add Panel → Select **GPU Server Prometheus** datasource
3. Enter PromQL query
4. Customize visualization
5. Save dashboard

## Alerts (Optional)

You can configure alerts in Grafana for:
- High GPU temperature (>85°C)
- High memory usage (>95%)
- Model offline (model_loaded == 0)
- High inference latency (>10s)

## Troubleshooting

### Grafana can't connect to Prometheus
- Check GPU server is accessible: `curl http://192.168.0.145:9090/api/v1/query?query=up`
- Verify Prometheus is running: `ssh ash "docker ps | grep prometheus"`

### No data showing
- Wait 15-30 seconds for first scrape
- Check Prometheus targets: http://192.168.0.145:9090/targets

### Dashboard not loading
- Check Grafana logs: `docker logs local-ai-grafana`
- Verify provisioning files are mounted correctly
