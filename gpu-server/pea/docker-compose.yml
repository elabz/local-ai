# HeartCode GPU Server - PEA Configuration
# 8x NVIDIA P104-100 (8GB each), 32GB RAM, 2-core Celeron
#
# Layout:
#   GPU 0-2: SFW Chat (Stheno v3.4 Q5_K_M) + Embeddings (3 instances)
#   GPU 3-6: NSFW Chat (Lumimaid v0.2 Q5_K_M) + Embeddings (4 instances)
#   GPU 7:   Image generation (Segmind SSD-1B via LocalAI)
#
# Context: 16K tokens (N_CTX=16384) â€” uses ~1GB KV cache at q8_0
# VRAM budget: ~5.7GB model + ~1GB KV + ~0.35GB embed + ~0.55GB overhead = ~7.6GB / 8GB
#
# Memory Budget (32GB total):
#   - 7x chat containers: 2GB each = 14GB max
#   - 7x embed containers: 768MB each = 5.25GB max
#   - 1x image container: 4GB max
#   - Note: mem_limit is per-container max, not reserved. Actual use much lower.
#   - Swap available for spikes
#
# Usage:
#   docker compose up -d
#   docker compose logs -f

x-gpu-server-common: &gpu-server-common
  image: local-ai-llama:latest
  restart: unless-stopped
  volumes:
    - ./models:/models:ro
    - ../configs:/app/configs:ro
    - ../server.py:/app/server.py:ro
    - ../metrics.py:/app/metrics.py:ro
    - ../routes.py:/app/routes.py:ro
    - ../llama_client.py:/app/llama_client.py:ro
    - ../config.py:/app/config.py:ro
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    interval: 15s
    timeout: 5s
    retries: 2
    start_period: 300s
  networks:
    - gpu-network
  mem_limit: 2048m
  memswap_limit: 3072m

x-gpu-env-common: &gpu-env-common
  N_GPU_LAYERS: 33
  N_CTX: 16384
  N_BATCH: 128
  N_UBATCH: 64
  N_THREADS: 2
  CACHE_REUSE: 256
  CACHE_TYPE_K: q8_0
  CACHE_TYPE_V: q8_0
  PORT: 8080

x-embedding-server-common: &embedding-server-common
  image: local-ai-llama:latest
  restart: unless-stopped
  volumes:
    - ./models:/models:ro
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
    interval: 15s
    timeout: 5s
    retries: 2
    start_period: 60s
  networks:
    - gpu-network
  mem_limit: 768m
  memswap_limit: 1024m
  entrypoint: ["llama-server"]
  command:
    - "--model"
    - "/models/nomic-embed-text-v1.5.Q8_0.gguf"
    - "--port"
    - "8090"
    - "--host"
    - "0.0.0.0"
    - "--embedding"
    - "--ctx-size"
    - "2048"
    - "--batch-size"
    - "512"
    - "--n-gpu-layers"
    - "99"
    - "--cache-type-k"
    - "q8_0"
    - "--cache-type-v"
    - "q8_0"

services:
  # =============================================================================
  # SFW CHAT SERVERS (GPUs 0-2)
  # =============================================================================

  gpu-server-1:
    <<: *gpu-server-common
    container_name: pea-gpu-1
    runtime: nvidia
    environment:
      <<: *gpu-env-common
      NVIDIA_VISIBLE_DEVICES: 0
      SERVER_ID: gpu-1
      MODEL_PATH: ${GPU_1_MODEL_PATH:-/models/model.gguf}
      MODEL_TYPE: ${GPU_1_MODEL_TYPE:-sfw}
      MODEL_NAME: ${GPU_1_MODEL_NAME:-unknown}
    ports:
      - "8080:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  gpu-server-2:
    <<: *gpu-server-common
    container_name: pea-gpu-2
    runtime: nvidia
    environment:
      <<: *gpu-env-common
      NVIDIA_VISIBLE_DEVICES: 1
      SERVER_ID: gpu-2
      MODEL_PATH: ${GPU_2_MODEL_PATH:-/models/model.gguf}
      MODEL_TYPE: ${GPU_2_MODEL_TYPE:-sfw}
      MODEL_NAME: ${GPU_2_MODEL_NAME:-unknown}
    ports:
      - "8081:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  gpu-server-3:
    <<: *gpu-server-common
    container_name: pea-gpu-3
    runtime: nvidia
    environment:
      <<: *gpu-env-common
      NVIDIA_VISIBLE_DEVICES: 2
      SERVER_ID: gpu-3
      MODEL_PATH: ${GPU_3_MODEL_PATH:-/models/model.gguf}
      MODEL_TYPE: ${GPU_3_MODEL_TYPE:-sfw}
      MODEL_NAME: ${GPU_3_MODEL_NAME:-unknown}
    ports:
      - "8082:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]

  # =============================================================================
  # NSFW CHAT SERVERS (GPUs 3-6)
  # =============================================================================

  gpu-server-4:
    <<: *gpu-server-common
    container_name: pea-gpu-4
    runtime: nvidia
    environment:
      <<: *gpu-env-common
      NVIDIA_VISIBLE_DEVICES: 3
      SERVER_ID: gpu-4
      MODEL_PATH: ${GPU_4_MODEL_PATH:-/models/model.gguf}
      MODEL_TYPE: ${GPU_4_MODEL_TYPE:-nsfw}
      MODEL_NAME: ${GPU_4_MODEL_NAME:-unknown}
    ports:
      - "8083:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']
              capabilities: [gpu]

  gpu-server-5:
    <<: *gpu-server-common
    container_name: pea-gpu-5
    runtime: nvidia
    environment:
      <<: *gpu-env-common
      NVIDIA_VISIBLE_DEVICES: 4
      SERVER_ID: gpu-5
      MODEL_PATH: ${GPU_5_MODEL_PATH:-/models/model.gguf}
      MODEL_TYPE: ${GPU_5_MODEL_TYPE:-nsfw}
      MODEL_NAME: ${GPU_5_MODEL_NAME:-unknown}
    ports:
      - "8084:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['4']
              capabilities: [gpu]

  gpu-server-6:
    <<: *gpu-server-common
    container_name: pea-gpu-6
    runtime: nvidia
    environment:
      <<: *gpu-env-common
      NVIDIA_VISIBLE_DEVICES: 5
      SERVER_ID: gpu-6
      MODEL_PATH: ${GPU_6_MODEL_PATH:-/models/model.gguf}
      MODEL_TYPE: ${GPU_6_MODEL_TYPE:-nsfw}
      MODEL_NAME: ${GPU_6_MODEL_NAME:-unknown}
    ports:
      - "8085:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['5']
              capabilities: [gpu]

  gpu-server-7:
    <<: *gpu-server-common
    container_name: pea-gpu-7
    runtime: nvidia
    environment:
      <<: *gpu-env-common
      NVIDIA_VISIBLE_DEVICES: 6
      SERVER_ID: gpu-7
      MODEL_PATH: ${GPU_7_MODEL_PATH:-/models/model.gguf}
      MODEL_TYPE: ${GPU_7_MODEL_TYPE:-nsfw}
      MODEL_NAME: ${GPU_7_MODEL_NAME:-unknown}
    ports:
      - "8086:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['6']
              capabilities: [gpu]

  # =============================================================================
  # EMBEDDING SERVERS (GPUs 0-6, co-located with chat servers)
  # =============================================================================

  embedding-server-1:
    <<: *embedding-server-common
    container_name: pea-embed-1
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: 0
    ports:
      - "8090:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  embedding-server-2:
    <<: *embedding-server-common
    container_name: pea-embed-2
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: 1
    ports:
      - "8091:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  embedding-server-3:
    <<: *embedding-server-common
    container_name: pea-embed-3
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: 2
    ports:
      - "8092:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]

  embedding-server-4:
    <<: *embedding-server-common
    container_name: pea-embed-4
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: 3
    ports:
      - "8093:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']
              capabilities: [gpu]

  embedding-server-5:
    <<: *embedding-server-common
    container_name: pea-embed-5
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: 4
    ports:
      - "8094:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['4']
              capabilities: [gpu]

  embedding-server-6:
    <<: *embedding-server-common
    container_name: pea-embed-6
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: 5
    ports:
      - "8095:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['5']
              capabilities: [gpu]

  embedding-server-7:
    <<: *embedding-server-common
    container_name: pea-embed-7
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: 6
    ports:
      - "8096:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['6']
              capabilities: [gpu]

  # =============================================================================
  # IMAGE GENERATION SERVER (GPU 7)
  # Segmind SSD-1B via LocalAI
  # =============================================================================

  image-server:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    container_name: pea-image-1
    restart: unless-stopped
    runtime: nvidia
    entrypoint: ["/config/entrypoint-wrapper.sh"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=7
      - DEBUG=true
      - THREADS=2
      - MODELS_PATH=/models
      - IMAGE_PATH=/tmp/generated
      - SINGLE_ACTIVE_BACKEND=true
    volumes:
      - ./models:/models
      - ./configs:/config:ro
      - image_cache:/tmp/generated
    ports:
      - "5100:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    networks:
      - gpu-network
    mem_limit: 4096m
    memswap_limit: 6144m
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['7']
              capabilities: [gpu]

  # =============================================================================
  # MONITORING
  # =============================================================================

  prometheus:
    image: prom/prometheus:latest
    container_name: pea-prometheus
    restart: unless-stopped
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9099:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - gpu-network

  node-exporter:
    image: prom/node-exporter:latest
    container_name: pea-node-exporter
    restart: unless-stopped
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "9100:9100"
    network_mode: host

  dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    container_name: pea-dcgm-exporter
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "9400:9400"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  gpu-network:
    driver: bridge

volumes:
  prometheus_data:
  image_cache:
