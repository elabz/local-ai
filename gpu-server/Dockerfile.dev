# Development Dockerfile - uses pre-built llama.cpp
# For faster iteration during development

FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

# Install dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Download pre-built llama.cpp server (update version as needed)
ARG LLAMA_CPP_VERSION=b4250
RUN wget -q https://github.com/ggerganov/llama.cpp/releases/download/${LLAMA_CPP_VERSION}/llama-${LLAMA_CPP_VERSION}-bin-ubuntu-x64-cuda-cu11.8.0.tar.gz \
    && tar -xzf llama-*.tar.gz \
    && mv build/bin/* /usr/local/bin/ \
    && rm -rf llama-*.tar.gz build

WORKDIR /app
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

COPY . .

RUN mkdir -p /models

ENV MODEL_PATH=/models/model.gguf
ENV HOST=0.0.0.0
ENV PORT=8080
ENV N_GPU_LAYERS=99
ENV N_CTX=4096

EXPOSE 8080

CMD ["python3", "server.py"]
