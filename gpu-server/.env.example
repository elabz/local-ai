# HeartCode GPU Server - PEA Model Configuration
# =============================================================================
# 8x NVIDIA P104-100 (8GB VRAM each)
#
# GPU 1-3: SFW (Stheno-L3.1-8B Q5_K_M)
# GPU 4-7: NSFW (Lumimaid v0.2 8B Q5_K_M)
# GPU 8:   Image generation (Segmind SSD-1B, managed by LocalAI)
#
# To apply changes:
#   1. Copy to .env:  cp .env.example .env
#   2. Run: docker compose up -d
# =============================================================================

# LiteLLM Configuration (for local testing)
LITELLM_MASTER_KEY=sk-change-me

# =============================================================================
# GPU 1-3: SFW Chat Models
# =============================================================================

GPU_1_MODEL_TYPE=sfw
GPU_1_MODEL_PATH=/models/Llama-3.1-8B-Stheno-v3.4-Q5_K_M.gguf
GPU_1_MODEL_NAME=Sao10K/Llama-3.1-8B-Stheno-v3.4

GPU_2_MODEL_TYPE=sfw
GPU_2_MODEL_PATH=/models/Llama-3.1-8B-Stheno-v3.4-Q5_K_M.gguf
GPU_2_MODEL_NAME=Sao10K/Llama-3.1-8B-Stheno-v3.4

GPU_3_MODEL_TYPE=sfw
GPU_3_MODEL_PATH=/models/Llama-3.1-8B-Stheno-v3.4-Q5_K_M.gguf
GPU_3_MODEL_NAME=Sao10K/Llama-3.1-8B-Stheno-v3.4

# =============================================================================
# GPU 4-7: NSFW Chat Models
# =============================================================================

GPU_4_MODEL_TYPE=nsfw
GPU_4_MODEL_PATH=/models/Lumimaid-v0.2-8B-Q5_K_M-imat.gguf
GPU_4_MODEL_NAME=NeverSleep/Lumimaid-v0.2-8B

GPU_5_MODEL_TYPE=nsfw
GPU_5_MODEL_PATH=/models/Lumimaid-v0.2-8B-Q5_K_M-imat.gguf
GPU_5_MODEL_NAME=NeverSleep/Lumimaid-v0.2-8B

GPU_6_MODEL_TYPE=nsfw
GPU_6_MODEL_PATH=/models/Lumimaid-v0.2-8B-Q5_K_M-imat.gguf
GPU_6_MODEL_NAME=NeverSleep/Lumimaid-v0.2-8B

GPU_7_MODEL_TYPE=nsfw
GPU_7_MODEL_PATH=/models/Lumimaid-v0.2-8B-Q5_K_M-imat.gguf
GPU_7_MODEL_NAME=NeverSleep/Lumimaid-v0.2-8B

# GPU 8: Image generation â€” managed by LocalAI, no model path config needed
