# GPU Server Configuration

# Server settings
HOST=0.0.0.0
PORT=8080
SERVER_ID=gpu-1

# Model configuration
MODEL_PATH=/models/stheno-8b-q4_k_m.gguf
N_GPU_LAYERS=33
N_CTX=4096
N_BATCH=512
N_THREADS=4

# llama.cpp server settings
LLAMA_SERVER_HOST=127.0.0.1
LLAMA_SERVER_PORT=8081

# Memory settings (for Pascal GPUs)
# P106-100: 6GB VRAM - use Q4_K_M
# P104-100: 8GB VRAM - can use Q5_K_M
CUDA_VISIBLE_DEVICES=0

# Inference settings
DEFAULT_TEMPERATURE=0.8
DEFAULT_TOP_P=0.95
DEFAULT_TOP_K=40
DEFAULT_REPEAT_PENALTY=1.1
DEFAULT_MAX_TOKENS=512

# Rate limiting
MAX_CONCURRENT_REQUESTS=4
REQUEST_TIMEOUT=120

# Metrics
ENABLE_METRICS=true
METRICS_PORT=9091

# Logging
LOG_LEVEL=INFO
