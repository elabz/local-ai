# LiteLLM Proxy Configuration - DEVELOPMENT (points to remote ASH GPU servers)
# Used for local development/testing with Langfuse integration
#
# Model Groups:
#   - local-ai-chat-sfw: GPUs 1-4 (safe for work models)
#   - local-ai-chat-nsfw: GPUs 5-8 (18+ models)

model_list:
  # =============================================================================
  # SFW MODELS - GPUs 1-4 (Remote ASH servers at 192.168.0.145)
  # =============================================================================

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu1"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 1
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8081/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu2"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 2
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8082/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu3"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 3
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8083/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu4"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 4
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  # =============================================================================
  # NSFW MODELS - GPUs 5-8 (Remote ASH servers at 192.168.0.145)
  # =============================================================================

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8084/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu5"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 5
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8085/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu6"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 6
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8086/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu7"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 7
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://192.168.0.145:8087/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu8"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 8
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"

# Router settings for load balancing
router_settings:
  routing_strategy: "least-busy"
  num_retries: 2
  retry_after: 3
  timeout: 90
  allowed_fails: 2
  cooldown_time: 60
  enable_health_check: true
  health_check_interval: 15

general_settings:
  master_key: "sk-O5z4E7s-6NhLDG3ZZbq9tQ"
  max_parallel_requests: 8
  global_max_parallel_requests: 12
  request_timeout: 90
  json_logs: true
  allow_credentials: true
  alerting: []

# Langfuse callbacks for observability
litellm_settings:
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  set_verbose: false
  drop_params: true
