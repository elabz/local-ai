# Prometheus configuration for HeartCode GPU monitoring

global:
  scrape_interval: 15s
  evaluation_interval: 15s

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: ["192.168.0.161:9093"]

# Alert rules
rule_files:
  - alert_rules.yml

scrape_configs:
  # GPU Server 1
  - job_name: 'gpu-server-1'
    static_configs:
      - targets: ['local-ai-gpu-1:9091']
        labels:
          gpu_id: '0'

  # GPU Server 2
  - job_name: 'gpu-server-2'
    static_configs:
      - targets: ['local-ai-gpu-2:9091']
        labels:
          gpu_id: '1'

  # GPU Server 3
  - job_name: 'gpu-server-3'
    static_configs:
      - targets: ['local-ai-gpu-3:9091']
        labels:
          gpu_id: '2'

  # GPU Server 4
  - job_name: 'gpu-server-4'
    static_configs:
      - targets: ['local-ai-gpu-4:9091']
        labels:
          gpu_id: '3'

  # GPU Server 5
  - job_name: 'gpu-server-5'
    static_configs:
      - targets: ['local-ai-gpu-5:9091']
        labels:
          gpu_id: '4'

  # GPU Server 6
  - job_name: 'gpu-server-6'
    static_configs:
      - targets: ['local-ai-gpu-6:9091']
        labels:
          gpu_id: '5'

  # GPU Server 7
  - job_name: 'gpu-server-7'
    static_configs:
      - targets: ['local-ai-gpu-7:9091']
        labels:
          gpu_id: '6'

  # GPU Server 8
  - job_name: 'gpu-server-8'
    static_configs:
      - targets: ['local-ai-gpu-8:9091']
        labels:
          gpu_id: '7'

  # LiteLLM Proxy
  - job_name: 'litellm'
    static_configs:
      - targets: ['local-ai-litellm:8000']
    metrics_path: '/metrics'

  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node Exporter for CPU/system metrics
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['192.168.0.145:9100']
        labels:
          instance: 'gpu-server'
