# LiteLLM Proxy Configuration for HeartCode
# Load balances between 8 GPU servers with SFW/NSFW model separation
#
# Model Groups:
#   - local-ai-chat-sfw: GPUs 1-4 (safe for work models)
#   - local-ai-chat-nsfw: GPUs 5-8 (18+ models)
#
# To change GPU assignments, edit .env and restart containers

model_list:
  # =============================================================================
  # SFW MODELS - GPUs 1-4
  # Request with model: "local-ai-chat-sfw"
  # =============================================================================

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-1:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu1"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 1
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-2:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu2"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 2
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-3:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu3"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 3
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  - model_name: "local-ai-chat-sfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-4:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu4"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 4
      model_name: "Sao10K/Stheno-L3.1-8B-GGUF"

  # =============================================================================
  # NSFW MODELS - GPUs 5-8
  # Request with model: "local-ai-chat-nsfw"
  #
  # RATE LIMITING: NSFW models are limited to 5 concurrent requests max due to
  # VRAM constraints on P106-100 (6GB). This is a capacity limit at 10 VU concurrency.
  # See NSFW-GPU-CAPACITY-INVESTIGATION.md for details.
  #
  # All 4 NSFW GPUs are registered for failover/load distribution, but the router
  # will respect the max_parallel_requests limit per model_name group.
  # =============================================================================

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-5:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu5"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 5
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"    # Tag for rate limiting

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-6:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu6"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 6
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"    # Tag for rate limiting

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-7:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu7"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 7
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"    # Tag for rate limiting

  - model_name: "local-ai-chat-nsfw"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-8:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-gpu8"
      mode: "chat"
      model_type: "nsfw"
      gpu_index: 8
      model_name: "Lewdiculous/Lumimaid-v0.2-8B-GGUF"
      rate_limit_group: "nsfw"    # Tag for rate limiting

  # =============================================================================
  # LEGACY ALIAS - Routes "local-ai-chat" to SFW models for backward compat
  # =============================================================================

  - model_name: "local-ai-chat"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-1:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-legacy-gpu1"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 1

  - model_name: "local-ai-chat"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-2:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-legacy-gpu2"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 2

  - model_name: "local-ai-chat"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-3:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-legacy-gpu3"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 3

  - model_name: "local-ai-chat"
    litellm_params:
      model: "openai/model"
      api_base: "http://gpu-server-4:8080/v1"
      api_key: "sk-local"
      max_tokens: 512
      temperature: 0.8
    model_info:
      id: "local-ai-legacy-gpu4"
      mode: "chat"
      model_type: "sfw"
      gpu_index: 4

  # =============================================================================
  # EMBEDDING MODELS - Load balanced across 8 GPU servers
  # Each GPU runs both chat model (~5.3GB) + embedding model (~350MB)
  # =============================================================================

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-1:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed1"
      mode: "embedding"

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-2:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed2"
      mode: "embedding"

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-3:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed3"
      mode: "embedding"

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-4:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed4"
      mode: "embedding"

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-5:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed5"
      mode: "embedding"

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-6:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed6"
      mode: "embedding"

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-7:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed7"
      mode: "embedding"

  - model_name: "local-ai-embed"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-8:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed8"
      mode: "embedding"

  # Also register as default text-embedding model for OpenAI compatibility
  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-1:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias1"
      mode: "embedding"

  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-2:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias2"
      mode: "embedding"

  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-3:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias3"
      mode: "embedding"

  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-4:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias4"
      mode: "embedding"

  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-5:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias5"
      mode: "embedding"

  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-6:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias6"
      mode: "embedding"

  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-7:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias7"
      mode: "embedding"

  - model_name: "text-embedding-nomic"
    litellm_params:
      model: "openai/nomic-embed-text-v1.5"
      api_base: "http://embedding-server-8:8090/v1"
      api_key: "sk-local"
      extra_body:
        encoding_format: "float"
    model_info:
      id: "local-ai-embed-alias8"
      mode: "embedding"

# Router settings for load balancing
# Configured for 30 rpm from 10 users (0.5 req/sec total, ~3 rpm per GPU)
#
# NSFW RATE LIMITING STRATEGY:
# - SFW (local-ai-chat-sfw): 8 concurrent max (4 GPUs × 2 concurrent each)
# - NSFW (local-ai-chat-nsfw): 5 concurrent max (4 GPUs × 1.25 concurrent avg)
# - Strategy: Allow 5 concurrent NSFW requests total across all 4 NSFW GPUs
# - Excess requests are queued by LiteLLM and served when capacity available
# - This prevents VRAM exhaustion on P106-100 (6GB) at high concurrency
#
router_settings:
  routing_strategy: "least-busy"  # Distribute by actual queue depth, not just round-robin

  # Retry settings - fail fast instead of hanging
  num_retries: 2                # Max 2 retries to different GPUs
  retry_after: 3                # 3 second delay before retry (allows temp recovery)
  timeout: 90                   # 90 second timeout per request (matches backend)

  # Unhealthy server detection (more aggressive for NSFW to prevent cascades)
  allowed_fails: 2              # Mark unhealthy after 2 consecutive failures
  cooldown_time: 60             # Don't retry unhealthy GPU for 60s (watchdog time)

  # Health check settings
  enable_health_check: true
  health_check_interval: 15     # 15 second health check (faster than Docker's 30s)

  # Rate limiting at proxy level (optional Redis-based rate limiting)
  redis_host: null              # Set if using Redis for distributed rate limiting
  redis_port: null
  redis_password: null

# Global rate limits to prevent memory exhaustion
general_settings:
  # Master key for API authentication (simple static key, no database required)
  # Matches INFERENCE_API_KEY in HeartCode backend docker-compose
  master_key: "sk-O5z4E7s-6NhLDG3ZZbq9tQ"

  # CRITICAL: Limit concurrent requests to prevent OOM
  # 8 GPUs available:
  #   - SFW: 4 GPUs, 8 concurrent max (2 per GPU)
  #   - NSFW: 4 GPUs, 5 concurrent max (1.25 per GPU) - VRAM capacity limit
  #   - Total global: 8 concurrent (SFW + NSFW combined)
  #   - Total with buffer: 12 (small headroom for failover)
  #
  # NSFW Rate Limiting:
  # - NSFW is rate-limited to 5 concurrent due to P106-100 VRAM exhaustion
  #   at >10 VU concurrency (see NSFW-GPU-CAPACITY-INVESTIGATION.md)
  # - At 5 concurrent: NSFW achieves 92.9% success
  # - At 10 concurrent: NSFW fails to 19.5% due to VRAM exhaustion
  # - LiteLLM will queue excess NSFW requests until capacity available
  #
  # DO NOT INCREASE without testing - risks OOM crashes
  max_parallel_requests: 8      # SFW (8) + NSFW (5) total = 13, limited to 8 global
  global_max_parallel_requests: 12 # 50% headroom (never more than this)

  # Request timeout (matches backend timeout)
  request_timeout: 90

  # Logging
  json_logs: true

  # CORS
  allow_credentials: true

  # Alerting (optional)
  alerting: []

# Callbacks for monitoring
# Langfuse integration for LLM observability
litellm_settings:
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]
  set_verbose: false
  drop_params: true  # Drop unsupported params instead of erroring

# Langfuse configuration
# Set these environment variables in docker-compose or .env:
#   LANGFUSE_PUBLIC_KEY=pk-lf-xxx (get from Langfuse UI after setup)
#   LANGFUSE_SECRET_KEY=sk-lf-xxx (get from Langfuse UI after setup)
#   LANGFUSE_HOST=http://langfuse-web:3000 (internal docker network)
