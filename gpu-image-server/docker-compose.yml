# HeartCode GPU Image Server
# LocalAI with Stable Diffusion for AI avatar generation
#
# Hardware: 2x NVIDIA RTX 3070 (8GB VRAM each)
#
# Layout:
#   GPU 0: Primary image generation server
#   GPU 1: Secondary (load balancing / failover)
#
# Usage:
#   docker compose up -d
#   docker compose logs -f

services:
  # =============================================================================
  # LOCALAI IMAGE GENERATION SERVERS
  # =============================================================================

  image-server-1:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    container_name: local-ai-image-1
    restart: unless-stopped
    runtime: nvidia
    entrypoint: ["/config/entrypoint-wrapper.sh"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - DEBUG=true
      - THREADS=4
      - MODELS_PATH=/models
      - IMAGE_PATH=/tmp/generated
      - SINGLE_ACTIVE_BACKEND=true
    volumes:
      - ./models:/models
      - ./configs:/config:ro
      - image_cache_1:/tmp/generated
      - backends:/backends
    ports:
      - "5000:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s  # SD models take time to load
    networks:
      - image-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  image-server-2:
    image: localai/localai:latest-gpu-nvidia-cuda-12
    container_name: local-ai-image-2
    restart: unless-stopped
    runtime: nvidia
    entrypoint: ["/config/entrypoint-wrapper.sh"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - DEBUG=true
      - THREADS=4
      - MODELS_PATH=/models
      - IMAGE_PATH=/tmp/generated
      - SINGLE_ACTIVE_BACKEND=true
    volumes:
      - ./models:/models
      - ./configs:/config:ro
      - image_cache_2:/tmp/generated
      - backends:/backends
    ports:
      - "5001:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    networks:
      - image-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  # =============================================================================
  # NGINX LOAD BALANCER
  # =============================================================================

  image-proxy:
    image: nginx:alpine
    container_name: local-ai-image-proxy
    restart: unless-stopped
    volumes:
      - ./configs/nginx.conf:/etc/nginx/nginx.conf:ro
      - image_cache_1:/generated-images-1:ro
      - image_cache_2:/generated-images-2:ro
    ports:
      - "${IMAGE_API_PORT:-5100}:80"
    depends_on:
      - image-server-1
      - image-server-2
    networks:
      - image-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:80/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  # =============================================================================
  # MONITORING
  # =============================================================================

  prometheus:
    image: prom/prometheus:latest
    container_name: local-ai-image-prometheus
    restart: unless-stopped
    volumes:
      - ./configs/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9091:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=15d'
    networks:
      - image-network

  node-exporter:
    image: prom/node-exporter:latest
    container_name: local-ai-image-node-exporter
    restart: unless-stopped
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    ports:
      - "9100:9100"
    network_mode: host

  dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    container_name: local-ai-image-dcgm
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "9400:9400"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  image-network:
    driver: bridge

volumes:
  image_cache_1:
  image_cache_2:
  prometheus_data:
  backends:
